# Credit Card Customer Churn Analysis

A pedagogical data science project analyzing credit card customer attrition patterns using Python, pandas, and Streamlit for interactive visualization.

## Project Overview

This project demonstrates a streamlined data science workflow from raw data to interactive dashboard. Using real-world credit card customer data, we analyze churn patterns and present insights through an interactive Streamlit dashboard. The project emphasizes simplicity, code quality, and educational value.

**Dataset**: [Credit Card Customers](https://www.kaggle.com/datasets/sakshigoyal7/credit-card-customers) from Kaggle
**Size**: 10,127 customers √ó 23 features
**Target Variable**: Attrition_Flag (Existing Customer vs Attrited Customer)

### Project Goals

‚úÖ **Simple ETL Pipeline**
- Clean, well-documented data processing script
- Parquet file format for efficient storage
- Single command execution

‚úÖ **Exploratory Data Analysis**
- Interactive visualization of distributions and relationships
- Dynamic filtering and segmentation
- Statistical summaries and correlation analysis

‚úÖ **Interactive Dashboard**
- Multi-tab Streamlit application
- Real-time filtering across 10k+ records
- Professional visualizations with Plotly

‚úÖ **Code Quality**
- Beginner-friendly extensive comments
- Linting (Ruff) and formatting (Black)
- Clean, maintainable codebase

## Project Structure

```
‚îú‚îÄ‚îÄ .github/
‚îÇ   ‚îî‚îÄ‚îÄ workflows/          # CI/CD pipelines (future)
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/               # Raw CSV data (committed to repo)
‚îÇ   ‚îî‚îÄ‚îÄ processed/         # Cleaned parquet file (generated by ETL)
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ etl.py             # ETL pipeline script
‚îú‚îÄ‚îÄ app.py                 # Streamlit dashboard application
‚îú‚îÄ‚îÄ requirements.txt       # Python dependencies
‚îú‚îÄ‚îÄ Makefile              # Development automation
‚îî‚îÄ‚îÄ README.md             # Project documentation
```

## Quick Start

### Prerequisites

- Python 3.12+ (or compatible version)
- Git for version control

### Setup & Run

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd Credit-Card-Customers
   ```

2. **Create and activate virtual environment**
   ```bash
   python -m venv .venv
   source .venv/bin/activate  # On Windows: .venv\Scripts\activate
   ```

3. **Install dependencies**
   ```bash
   make install
   # or: pip install -r requirements.txt
   ```

4. **Run the dashboard**
   ```bash
   make app
   # or: streamlit run app.py
   ```

   The dashboard will automatically run the ETL pipeline if needed! üéâ

### Optional: Run ETL Manually

If you want to regenerate the processed data:
```bash
make etl
# or: python src/etl.py
```

## Data Pipeline

Our simplified pipeline follows this flow:

```
Raw CSV ‚Üí ETL Processing ‚Üí Parquet ‚Üí Dashboard
  ‚Üì           ‚Üì              ‚Üì          ‚Üì
data/raw  src/etl.py   data/processed  app.py
```

### ETL Process (`src/etl.py`)

The ETL script performs:

1. **Extract**: Load raw CSV from `data/raw/BankChurners.csv`
2. **Transform**:
   - Standardize column names (lowercase, underscores)
   - Enforce data types (integers, floats, strings)
   - Create derived features (churn_flag, avg_transaction, tenure_years)
   - Handle missing values
   - Remove duplicates
3. **Load**: Save as parquet to `data/processed/cleaned.parquet`

**Key Features**:
- Extensive pedagogical comments explaining each step
- Proper type handling (nullable integers, floats)
- Feature engineering (transaction averages, tenure conversion)
- Robust error handling

### Dashboard (`app.py`)

Interactive Streamlit application with 4 analytical views:

1. **üìä Overview**: Dataset summary, KPIs, data preview
2. **üìà Distributions**: Histograms, box plots, summary statistics
3. **üîç Churn Analysis**: Pie charts, cross-tabulations, segment comparisons
4. **üîó Correlations**: Heatmaps, scatter plots, relationship exploration

**Features**:
- Automatic ETL execution if data missing
- Real-time filtering (churn status, gender, card category)
- Interactive Plotly visualizations
- Responsive layout with tabs and columns

## Development Workflow

### Make Commands

```bash
make help         # Show all available commands
make install      # Install dependencies
make lint         # Run Ruff linting
make format       # Run Black code formatting
make pre-commit   # Run lint + format (recommended before commits)
make etl          # Run ETL pipeline
make app          # Run Streamlit dashboard
make clean        # Remove caches and temp files
```

### Code Quality Tools

**Ruff** - Fast Python linter
- Checks code quality and style
- Command: `make lint` or `ruff check .`

**Black** - Code formatter
- Ensures consistent code style
- Command: `make format` or `black .`

**Pre-commit Workflow**:
```bash
make pre-commit  # Run before committing changes
git add .
git commit -m "your message"
```

## Technology Stack

### Core Libraries

**pandas (2.3.3)** - Data manipulation
- DataFrame operations, cleaning, transformation
- Statistical summaries and aggregation

**numpy (2.0.2)** - Numerical computing
- Array operations and type handling

**pyarrow (21.0.0)** - Parquet support
- Efficient data storage (50-80% smaller than CSV)
- Preserves data types

### Visualization

**Streamlit (1.50.0)** - Dashboard framework
- Web application with zero frontend code
- `@st.cache_data` for performance optimization
- Interactive widgets (multiselect, slider, tabs)

**Plotly (6.3.1)** - Interactive charts
- Histograms, box plots, scatter plots, heatmaps
- Hover tooltips, zoom, and pan functionality

**Matplotlib (3.10.6) & Seaborn (0.13.2)** - Statistical plotting
- Additional visualization options

### Development Tools

**Ruff (0.8.4)** - Python linter (10-100x faster than alternatives)
**Black (24.10.0)** - Code formatter
**scikit-learn (1.6.1)** - Machine learning (future modeling)
**statsmodels (0.14.5)** - Statistical analysis (future modeling)

## Key Insights

### Dataset Overview

- **Total Records**: 10,127 customers
- **Features**: 23 variables (16 numeric, 7 categorical)
- **Churn Rate**: 16.1% (1,627 attrited customers)
- **Data Quality**: Zero missing values, no duplicates

### Churn Patterns

Based on dashboard exploration:

1. **Transaction Behavior**: Churned customers show lower transaction counts and amounts
2. **Card Distribution**: Blue Card holders constitute ~93% of customers
3. **Feature Relationships**: Strong correlations between transaction-related features

Explore these patterns interactively via the dashboard filters and visualizations!

## Educational Focus

This project is designed for collaborative learning with the following pedagogical features:

### For Beginners

- **Extensive Comments**: Every section of code explained in detail
- **Concept Explanations**: Python idioms and pandas operations documented
- **Simple Structure**: Avoids over-engineering, focuses on fundamentals
- **Progressive Complexity**: Start simple (data loading) ‚Üí build to advanced (interactive viz)

### Code Examples Covered

- List comprehensions and generator expressions
- Dictionary operations and comprehension
- DataFrame manipulation (filtering, groupby, pivot)
- Type handling (nullable types, type conversion)
- Function definition and modularity (`if __name__ == "__main__"`)
- Error handling (try-except blocks)
- String operations and regex patterns
- Path handling with pathlib

### Best Practices Demonstrated

- Version control with Git
- Code formatting and linting
- Documentation and comments
- Modular code organization
- Defensive programming (error handling)
- Performance optimization (caching)

## Deployment

### Streamlit Cloud (Recommended)

1. Push repository to GitHub
2. Connect to Streamlit Cloud
3. Configure app settings
4. Deploy with auto-reload on commits

### Local Deployment

Simply run:
```bash
streamlit run app.py
```

The app automatically handles data processing on first run.

## Future Enhancements

Potential extensions for advanced learning:

- **Machine Learning**: Build churn prediction models (Random Forest, XGBoost)
- **Model Interpretability**: Add SHAP values for feature importance
- **Customer Segmentation**: K-means clustering analysis
- **Time Series**: Analyze churn trends over time
- **API Integration**: RESTful API for model predictions
- **Containerization**: Docker setup for reproducible deployment

## Contributing

This is a pedagogical group project. Contributions welcome via:

- Feature branches with descriptive names
- Pull requests with clear descriptions
- Code reviews with constructive feedback
- Documentation improvements

**Pre-commit Checklist**:
```bash
make pre-commit  # Lint and format
git add .
git commit -m "feat: descriptive message"
```

## Acknowledgments

- **Dataset**: [Sakshi Goyal](https://www.kaggle.com/sakshigoyal7) via Kaggle
- **Framework**: Built for collaborative learning and skill development

## License

This project is for educational purposes.

---

**Need Help?**
- Run `make help` to see all available commands
- Check inline code comments for explanations
- Review this README for workflow guidance
