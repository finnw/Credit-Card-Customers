{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "0aStgWSO0E0E"
   },
   "source": [
    "# **01_extract**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "id": "1eLEkw5O0ECa"
   },
   "source": [
    "## Objectives\n",
    "\n",
    "* Fetch the credit card customer dataset directly from Kaggle using the prepared ingestion script.  \n",
    "* Store the dataset in the project's `data/raw/` folder so it is available for later stages of the ETL process.  \n",
    "* Capture the accompanying data dictionary (if provided) to help guide future cleaning and transformation.\n",
    "\n",
    "## Inputs\n",
    "\n",
    "* Access to the Kaggle platform with an API key configured (`kaggle.json` placed in `.kaggle/`).  \n",
    "* The dataset name defined inside the ingestion script (`sakshigoyal7/credit-card-customers`).  \n",
    "* Project directory structure with a `data/` folder to hold raw and interim data.\n",
    "\n",
    "## Outputs\n",
    "\n",
    "* A local copy of the Kaggle dataset saved under `data/raw/`.  \n",
    "* A JSON version of the dataset's data dictionary (if available), saved alongside the raw data.  \n",
    "* A Parquet file containing the initial snapshot of the dataset, saved under `data/interim/`.  \n",
    "* This interim Parquet file is committed to the GitHub repository, so it is **not necessary** to re-run the ingestion step if you prefer to begin directly with the transformation stage.  \n",
    "* A short console report showing the dataset location and initial structure (rows, columns, dtypes).\n",
    "\n",
    "## Additional Comments\n",
    "\n",
    "* This notebook does not analyse or transform the data; its role is only to fetch and stage it.  \n",
    "* The use of the data dictionary is intended to give structure to the dataset early, simplifying later validation and transformation.  \n",
    "* All subsequent notebooks in the ETL pipeline assume that this step has completed successfully and that the dataset is present in `data/raw/` and mirrored as an interim Parquet file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "id": "9uWZXH9LwoQg"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "id": "-mavJ8DibrcQ"
   },
   "source": [
    "# Data Ingestion from Kaggle\n",
    "\n",
    "This project uses a Python script (`ingest.py`) in the project root to download the dataset directly from Kaggle.  \n",
    "The script relies on the official Kaggle API and performs the following steps:\n",
    "\n",
    "- Downloads the dataset specified in `DATASET_NAME`.\n",
    "- Copies the raw files (CSV, data dictionary, etc.) into `<project-root>/data/raw/`.\n",
    "- If a data dictionary is provided (e.g. Excel), parses it into a JSON schema for later use.\n",
    "- Prints a summary report of rows, columns, and dtypes.\n",
    "\n",
    "⚠️ **Kaggle API key requirement:**  \n",
    "To use the Kaggle API, you must have a Kaggle account and create an API token.  \n",
    "1. Go to [https://www.kaggle.com/](https://www.kaggle.com/) → *Account* → *Create New API Token*.  \n",
    "2. This downloads a file called `kaggle.json`.  \n",
    "3. Place the `kaggle.json` file in the hidden directory: `/.kaggle/kaggle.json` (create the `.kaggle` folder if it doesn’t exist).  \n",
    "4. Ensure it has correct permissions (on Unix/Mac: `chmod 600 ~/.kaggle/kaggle.json`).\n",
    "\n",
    "Once configured, running the notebook cell that calls `ingest.py` will automatically fetch the dataset from Kaggle and populate the `data/raw/` directory for subsequent ETL steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Imports & Configuration\n",
    "\n",
    "All imports are kept together at the top of the notebook. This is a deliberate choice:\n",
    "\n",
    "* **Consistency with Python standards:** Ruff (our linter) enforces [PEP 8](https://peps.python.org/pep-0008/) which says all module-level imports should be at the start of a file. In notebooks, each `.ipynb` is effectively a Python module, so the same rule applies.\n",
    "* **Clarity for readers:** Having imports scattered across cells makes it hard to know which libraries are required to run the notebook. Consolidating them up-front provides a single, predictable place to look.\n",
    "* **Reproducibility:** When imports are at the top, you can quickly check your environment and install what’s missing. If imports are sprinkled throughout, you might only discover a missing package halfway through execution.\n",
    "* **Efficiency:** Importing a library multiple times in different cells doesn’t hurt Python (it caches modules), but it clutters the notebook and increases cognitive overhead.\n",
    "* **Tooling compatibility:** Linters and formatters (Ruff, nbQA) assume imports are at the top. Following this convention avoids noisy errors (e.g. E402) and makes automated checks pass smoothly.\n",
    "\n",
    "While many data science notebooks mix imports and code ad-hoc, we are treating this notebook like a real Python module: all imports first, then configuration, then data loading and analysis. This keeps our workflow clean and professional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports & config (keep imports here to satisfy Ruff E402)\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import pathlib\n",
    "import hashlib\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Run ingest.py from the project root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure out the project root: one directory up from this notebook\n",
    "project_root = Path.cwd().parent\n",
    "\n",
    "ingest_script = project_root / \"ingest.py\"\n",
    "\n",
    "# Run the ingest script using the same Python interpreter as the notebook\n",
    "result = subprocess.run(\n",
    "    [sys.executable, str(ingest_script)], capture_output=True, text=True\n",
    ")\n",
    "\n",
    "print(\"STDOUT:\\n\", result.stdout)\n",
    "print(\"STDERR:\\n\", result.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "# Load raw data + apply schema (from data dictionary)\n",
    "This cell demonstrates using the ingested Kaggle credit card customer dataset (CSV) and the derived schema JSON to build a structured pandas DataFrame with correct types, categories, and parsed dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DIR = pathlib.Path(\"../data/raw\")\n",
    "\n",
    "\n",
    "def _find_first_csv(raw_dir: pathlib.Path) -> str | None:\n",
    "    csvs = glob.glob(str(raw_dir / \"**\" / \"*.csv\"), recursive=True)\n",
    "    return csvs[0] if csvs else None\n",
    "\n",
    "\n",
    "def _schema_path_for_csv(csv_path: str) -> str | None:\n",
    "    \"\"\"We saved schema as: data/raw/<dataset_folder>_schema.json\"\"\"\n",
    "    folder = os.path.basename(os.path.dirname(csv_path))\n",
    "    candidate = RAW_DIR / f\"{folder}_schema.json\"\n",
    "    if candidate.exists():\n",
    "        return str(candidate)\n",
    "    # Fallback: first *_schema.json in raw\n",
    "    any_schema = glob.glob(str(RAW_DIR / \"*_schema.json\"))\n",
    "    return any_schema[0] if any_schema else None\n",
    "\n",
    "\n",
    "def _apply_schema(df: pd.DataFrame, schema: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Enforce schema-defined types, categories, and dates on the DataFrame.\n",
    "    The schema comes from the Kaggle data dictionary (parsed earlier and saved as JSON).\n",
    "    Using this structure up-front simplifies downstream cleaning, validation, and visuals.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    dtypes = schema.get(\"dtypes\", {})\n",
    "    cats = schema.get(\"categories\", {})\n",
    "    dates = schema.get(\"parse_dates\", [])\n",
    "    descs = schema.get(\"descriptions\", {})\n",
    "\n",
    "    # 1) Set base dtypes (string/boolean/Int64/float64). Categories handled after.\n",
    "    for col, dtype in dtypes.items():\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        try:\n",
    "            if dtype in (\"string\", \"boolean\", \"Int64\", \"float64\"):\n",
    "                df[col] = df[col].astype(dtype)\n",
    "        except Exception:\n",
    "            # Leave as-is if conversion fails; we’ll still try dates/categories below\n",
    "            pass\n",
    "\n",
    "    # 2) Parse dates\n",
    "    for col in dates:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "\n",
    "    # 3) Apply categoricals (ordered if provided)\n",
    "    for col, categories in cats.items():\n",
    "        if col in df.columns and categories:\n",
    "            try:\n",
    "                ctyp = pd.api.types.CategoricalDtype(\n",
    "                    categories=categories, ordered=True\n",
    "                )\n",
    "                df[col] = df[col].astype(ctyp)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    # 4) Attach column descriptions for reference (handy in later EDA cells)\n",
    "    df.attrs[\"dictionary\"] = descs\n",
    "    return df\n",
    "\n",
    "\n",
    "# ---- Load CSV + Schema and build the DataFrame ----\n",
    "csv_path = _find_first_csv(RAW_DIR)\n",
    "assert csv_path is not None, \"No CSV found in data/raw. Run the ingest step first.\"\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "schema_path = _schema_path_for_csv(csv_path)\n",
    "\n",
    "if schema_path:\n",
    "    with open(schema_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        schema = json.load(f)\n",
    "    df = _apply_schema(df, schema)\n",
    "    print(f\"Loaded CSV:\\n  {csv_path}\\nApplied schema:\\n  {schema_path}\")\n",
    "else:\n",
    "    print(f\"Loaded CSV (no schema JSON found):\\n  {csv_path}\")\n",
    "\n",
    "print(\"\\nShape:\", df.shape)\n",
    "print(\"\\nDtypes (first 12 columns):\")\n",
    "print(df.dtypes.head(12))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {
    "id": "ZY3l0-AxO93d"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Validation and Interim Data Snapshot\n",
    "\n",
    "At this stage we pause between **Extract** and **Transform** to check the integrity of the dataset and create a stable copy for future use.\n",
    "\n",
    "**What happens here:**\n",
    "- A **validation report** is generated in JSON format.  \n",
    "  - Confirms the dataset shape (rows, columns).  \n",
    "  - Records detected column types and compares them with the schema supplied in the data dictionary.  \n",
    "  - Flags potential issues such as missing values, duplicate IDs, negative values, or unparsed dates.  \n",
    "  - Captures a simple hash of sample data so we can track provenance.\n",
    "\n",
    "- A **Parquet file** is created under `data/interim/`.  \n",
    "  - This acts as a snapshot of the dataset immediately after ingestion and validation.  \n",
    "  - Parquet is chosen because it is efficient, preserves data types, and is well-suited for analytical workflows.\n",
    "\n",
    "**Why we do this:**\n",
    "- Ensures we have a trusted baseline of the data before making any changes.  \n",
    "- Provides a record we can revisit if transformations introduce errors.  \n",
    "- Allows collaborators (or future runs of this project) to skip the ingestion process and begin directly from a consistent, version-controlled dataset included in the GitHub repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume df already loaded in memory\n",
    "project_root = (\n",
    "    pathlib.Path.cwd().parent\n",
    ")  # notebook in jupyter_notebooks/, root is parent\n",
    "interim_dir = project_root / \"data\" / \"interim\"\n",
    "interim_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "report = {}\n",
    "\n",
    "# 1) Basics\n",
    "report[\"shape\"] = {\"rows\": int(df.shape[0]), \"cols\": int(df.shape[1])}\n",
    "report[\"columns\"] = df.columns.tolist()\n",
    "\n",
    "# 2) Schema alignment (if schema JSON exists)\n",
    "schema = {}\n",
    "raw_dir = project_root / \"data\" / \"raw\"\n",
    "schema_jsons = list(raw_dir.glob(\"*_schema.json\"))\n",
    "if schema_jsons:\n",
    "    schema_path = schema_jsons[0]\n",
    "    schema = json.loads(schema_path.read_text())\n",
    "    expected = (\n",
    "        set(schema.get(\"dtypes\", {}))\n",
    "        | set(schema.get(\"parse_dates\", []))\n",
    "        | set(schema.get(\"categories\", {}))\n",
    "    )\n",
    "    report[\"schema_alignment\"] = {\n",
    "        \"expected_present\": sorted(set(df.columns) & expected),\n",
    "        \"expected_missing\": sorted(expected - set(df.columns)),\n",
    "        \"unexpected_columns\": sorted(set(df.columns) - expected),\n",
    "    }\n",
    "else:\n",
    "    report[\"schema_alignment\"] = \"no_schema_json_found\"\n",
    "\n",
    "# 3) Dtypes summary\n",
    "report[\"dtypes\"] = {c: str(t) for c, t in df.dtypes.items()}\n",
    "\n",
    "# 4) Primary key check (simple heuristic)\n",
    "PRIMARY_KEY = next(\n",
    "    (c for c in df.columns if c.lower() in {\"clientnum\", \"customer_id\", \"id\"}), None\n",
    ")\n",
    "if PRIMARY_KEY:\n",
    "    report[\"primary_key\"] = {\n",
    "        \"column\": PRIMARY_KEY,\n",
    "        \"duplicates\": int(df[PRIMARY_KEY].duplicated().sum()),\n",
    "        \"nulls\": int(df[PRIMARY_KEY].isna().sum()),\n",
    "    }\n",
    "else:\n",
    "    report[\"primary_key\"] = \"no obvious key guessed\"\n",
    "\n",
    "# 5) Missingness map (%)\n",
    "null_pct = (df.isna().mean() * 100).round(2).sort_values(ascending=False)\n",
    "report[\"missingness_top10_pct\"] = null_pct.head(10).to_dict()\n",
    "\n",
    "# 6) Range checks\n",
    "num_cols = df.select_dtypes(\"number\").columns\n",
    "report[\"negative_value_columns\"] = [c for c in num_cols if (df[c] < 0).any()]\n",
    "\n",
    "# 7) Date sanity\n",
    "date_cols = [c for c, t in df.dtypes.items() if \"datetime64\" in str(t)]\n",
    "report[\"date_columns_na_counts\"] = {c: int(df[c].isna().sum()) for c in date_cols}\n",
    "\n",
    "# 8) Provenance hash\n",
    "sample_bytes = df.head(1000).to_csv(index=False).encode(\"utf-8\")\n",
    "report[\"provenance\"] = {\n",
    "    \"sample_sha1\": hashlib.sha1(sample_bytes).hexdigest(),\n",
    "    \"timestamp\": int(time.time()),\n",
    "}\n",
    "\n",
    "# Save the validation report JSON\n",
    "snapshot_path = interim_dir / \"pre_transform_validation.json\"\n",
    "snapshot_path.write_text(json.dumps(report, indent=2))\n",
    "\n",
    "# Save the interim DataFrame as Parquet\n",
    "parquet_path = interim_dir / \"pre_transform_snapshot.parquet\"\n",
    "df.to_parquet(parquet_path, index=False)\n",
    "\n",
    "print(\"Validation report →\", snapshot_path)\n",
    "print(\"Interim Parquet saved →\", parquet_path)\n",
    "\n",
    "pd.Series(report[\"dtypes\"]).head(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Data Practitioner Jupyter Notebook.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "jupytext": {
   "cell_metadata_filter": "tags,hide_input,hide_output,id",
   "formats": "ipynb,py:percent",
   "notebook_metadata_filter": "all,-widgets"
  },
  "kernelspec": {
   "display_name": ".venv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
